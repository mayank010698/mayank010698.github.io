---
---

@string{aps = {American Physical Society,}}


@InProceedings{pmlr-v201-lau23a,
  selected={true},
  title = 	 {Max-Quantile Grouped Infinite-Arm Bandits},
  author =       {Lau, Ivan and Ling, Yan Hao and Shrivastava, Mayank and Scarlett, Jonathan},
  booktitle = 	 {Proceedings of The 34th International Conference on Algorithmic Learning Theory},
  pages = 	 {909--945},
  year = 	 {2023},
  editor = 	 {Agrawal, Shipra and Orabona, Francesco},
  volume = 	 {201},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {20 Feb--23 Feb},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v201/lau23a/lau23a.pdf},
  url = 	 {https://proceedings.mlr.press/v201/lau23a.html},
  abstract = 	 {In this paper, we consider a bandit problem in which there are a number of groups each consisting of infinitely many arms.  Whenever a new arm is requested from a given group, its mean reward is drawn from an unknown reservoir distribution (different for each group), and the uncertainty in the armâ€™s mean reward can only be reduced via subsequent pulls of the arm.  The goal is to identify the infinite-arm group whose reservoir distribution has the highest $(1-\alpha)$-quantile (e.g., median if $\alpha = \frac{1}{2}$), using as few total arm pulls as possible.  We introduce a two-step algorithm that first requests a fixed number of arms from each group and then runs a finite-arm grouped max-quantile bandit algorithm.  We characterize both the instance-dependent and worst-case regret, and provide a matching lower bound for the latter, while discussing various strengths, weaknesses, algorithmic improvements, and potential lower bounds associated with our instance-dependent upper bounds.}
}

